from atom.api import Unicode, Typed, Property, Bool
from enaml.application import deferred_call
from enaml.layout.api import InsertItem
from enaml.widgets.api import DockItem
from enaml.workbench.api import Extension, PluginManifest
from enaml.workbench.core.api import Command

from traits_enaml.widgets.enable_canvas import EnableCanvas

import tables as tb
import pandas as pd
import numpy as np

from ..plugin import Sink
from psi import get_config
from psi.core.utils import find_extension
from psi.core.chaco.api import ChannelPlot, add_time_axis

from .data_channel import DataChannel

PLUGIN_ID = 'psi.data.hdf_store'


class HDFStore(Sink):
    '''
    Simple class for storing acquired trial data in a HDF5 file. No analysis or
    further processing is done.
    '''
    file_handle = Property()
    node = Typed(tb.Group)
    initialized = Bool(False)

    trial_log = Typed(tb.Table)
    trial_log_dtype = Typed(np.dtype)
    event_log = Typed(tb.Table)
    event_log_dtype = Typed(np.dtype)

    _channels = Typed(dict)

    def _get_file_handle(self):
        return self.node._v_file

    def _prepare_trial_log(self, context_info):
        '''
        Create a table to hold the event log.
        '''
        dtype = [(str(name), item['dtype']) for name, item \
                 in context_info.items()]
        self.trial_log_dtype = np.dtype(dtype)
        self.trial_log = self.file_handle.create_table(self.node,
                                                       'trial_log',
                                                       self.trial_log_dtype)

    def _prepare_event_log(self):
        '''
        Create a table to hold the event log.
        '''
        dtype = [('timestamp', np.dtype('float32')), 
                 ('event', np.dtype('S512'))]
        self.event_log_dtype = np.dtype(dtype)
        self.event_log = self.file_handle.create_table(self.node,
                                                       'event_log',
                                                       self.event_log_dtype)

    def _prepare_continuous_input(self, input):
        atom = tb.Atom.from_dtype(input.channel.dtype)
        expected_rows = int(input.channel.fs * 60 * 60)
        filters = tb.Filters(**get_config('H5_COMPRESSION'))
        node = self.file_handle.create_earray(self.node, input.name, atom,
                                              (0,), filters=filters,
                                              expectedrows=expected_rows)
        for name, value in input.__getstate__().items():
            node._v_attrs[name] = value
        for name, value in input.channel.__getstate__().items():
            node._v_attrs['channel_' + name] = value
        for name, value in input.engine.__getstate__().items():
            node._v_attrs['engine_' + name] = value
        return DataChannel(data=node, fs=input.fs) 

    def _prepare_event_input(self, input):
        # TODO!
        return None
        atom = tb.Float64Atom()
        filters = tb.Filters(**get_config('H5_COMPRESSION'))
        return self.file_handle.create_earray(self.node, output.name, atom,
                                              (0,), filters=filters,
                                              expectedrows=1000)

    def _prepare_inputs(self, inputs):
        channels = {}
        for input in inputs:
            prep_function_name = '_prepare_{}_input'.format(input.mode)
            prep_function = getattr(self, prep_function_name)
            channels[input.name] = prep_function(input)
        self._channels = channels

    def prepare(self, plugin):
        self._prepare_event_log()
        self._prepare_trial_log(plugin.context_info)
        self._prepare_inputs(plugin.inputs.values())
        self._prepare_plots()
        self.initialized = True

    def finalize(self):
        self.file_handle.close()

    def process_trial(self, results):
        # This is the simplest one-liner to convert the dictionary to the
        # format required for appending.
        row = pd.DataFrame([results]).to_records().astype(self.trial_log_dtype)
        self.trial_log.append(row)

    def process_event(self, event, timestamp):
        data = {'event': event, 'timestamp': timestamp}
        row = pd.DataFrame([data]).to_records().astype(self.event_log_dtype)
        self.event_log.append(row)

    def process_ai(self, name, data):
        if self._channels[name] is not None:
            self._channels[name].append(data)

    container = Typed(object)

    def _default_container(self):
        return OverlayPlotContainer(padding=[20, 20, 50, 5])

    def _prepare_plots(self):
        index_range = ChannelDataRange(trig_delay=0)
        index_mapper = LinearMapper(range=index_range)

        container = OverlayPlotContainer(padding=[20, 20, 50, 50])

        value_range = DataRange1D(low_setting=0, high_setting=5)
        value_mapper = LinearMapper(range=value_range)

        plot = ChannelPlot(source=self._channels['nose_poke_analog'],
                           index_mapper=index_mapper,
                           value_mapper=value_mapper, line_color='red')
        add_time_axis(plot)
        container.add(plot)
        self.container = container


def set_node(event):
    extension = find_extension(event.workbench, PLUGIN_ID, 'sink', HDFStore)
    if extension.initialized:
        raise ValueError('Cannot change node once data has initialized')
    extension.node = event.parameters['node']


from psi.core.chaco.api import ChannelDataRange
from chaco.api import LinearMapper, OverlayPlotContainer, DataRange1D

from enaml.widgets.api import Container, Label

enamldef ChacoDockItem(DockItem):

    name = 'chaco_dock_item'
    closable = False

    attr extension

    Container:
        EnableCanvas:
            component << extension.container


def contribute_to_workspace(workbench, workspace):
    extension = find_extension(workbench, PLUGIN_ID, 'sink', HDFStore)
    item = ChacoDockItem(workspace.dock_area, extension=extension)
    op = InsertItem(item=item.name, position='right')
    deferred_call(workspace.dock_area.update_layout, op)


enamldef HDFStoreManifest(PluginManifest): manifest:

    id = PLUGIN_ID

    Extension:
        id = 'sink'
        point = 'psi.data.sink'
        HDFStore:
            pass

    Extension:
        id = 'commands'
        point = 'enaml.workbench.core.commands'
        Command:
            id = PLUGIN_ID + '.set_node'
            handler = set_node

    Extension:
        id = 'workspace'
        point = 'psi.experiment.workspace'
        factory = contribute_to_workspace
