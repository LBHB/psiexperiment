from atom.api import Unicode, Typed, Property, Bool
from enaml.application import deferred_call
from enaml.layout.api import InsertItem
from enaml.widgets.api import DockItem
from enaml.workbench.api import Extension, PluginManifest
from enaml.workbench.core.api import Command

from traits_enaml.widgets.enable_canvas import EnableCanvas

import tables as tb
import pandas as pd
import numpy as np

from ..plugin import Sink
from psi import get_config
from psi.controller.api import ContinuousAnalogInput, AnalogThreshold
from psi.core.utils import find_extension

PLUGIN_ID = 'psi.data.hdf_store'


class Channel(object):


class HDFStore(Sink):
    '''
    Simple class for storing acquired trial data in a HDF5 file. No analysis or
    further processing is done.
    '''
    file_handle = Property()
    node = Typed(tb.Group)
    initialized = Bool(False)

    trial_log = Typed(tb.Table)
    trial_log_dtype = Typed(np.dtype)
    event_log = Typed(tb.Table)
    event_log_dtype = Typed(np.dtype)

    _arrays = Typed(dict)
    _channels = Typed(dict)

    def _get_file_handle(self):
        return self.node._v_file

    def _prepare_trial_log(self, context_info):
        '''
        Create a table to hold the event log.
        '''
        dtype = [(str(name), item['dtype']) for name, item \
                 in context_info.items()]
        self.trial_log_dtype = np.dtype(dtype)
        self.trial_log = self.file_handle.create_table(self.node,
                                                       'trial_log',
                                                       self.trial_log_dtype)

    def _prepare_event_log(self):
        '''
        Create a table to hold the event log.
        '''
        dtype = [('timestamp', np.dtype('float32')), 
                 ('event', np.dtype('S512'))]
        self.event_log_dtype = np.dtype(dtype)
        self.event_log = self.file_handle.create_table(self.node,
                                                       'event_log',
                                                       self.event_log_dtype)

    def _prepare_continuous_input(self, input):
        atom = tb.Atom.from_dtype(input.channel.dtype)
        expected_rows = int(input.channel.fs * 60 * 60)
        filters = tb.Filters(**get_config('H5_COMPRESSION'))
        node = self.file_handle.create_earray(self.node, input.name, atom,
                                              (0,), filters=filters,
                                              expectedrows=expected_rows)
        for name, value in input.__getstate__().items():
            node._v_attrs[name] = value
        for name, value in input.channel.__getstate__().items():
            node._v_attrs['channel_' + name] = value
        for name, value in input.engine.__getstate__().items():
            node._v_attrs['engine_' + name] = value
        return node

    def _prepare_epoch_input(self, input):
        return None
        atom = tb.Float64Atom()
        filters = tb.Filters(**get_config('H5_COMPRESSION'))
        return self.file_handle.create_earray(self.node, output.name, atom,
                                              (0,), filters=filters,
                                              expectedrows=1000)

    _input_map = {
        ContinuousAnalogInput: _prepare_continuous_input,
        AnalogThreshold: _prepare_epoch_input,
    }

    def _prepare_inputs(self, inputs):
        arrays = {}
        for input in inputs:
            prepare_function = self._input_map[type(input)]
            arrays[input.name] = prepare_function(self, input)
        self._arrays = arrays

    def prepare(self, plugin):
        self._prepare_event_log()
        self._prepare_trial_log(plugin.context_info)
        self._prepare_inputs(plugin.inputs.values())
        self.initialized = True

    def finalize(self):
        self.file_handle.close()

    def process_trial(self, results):
        # This is the simplest one-liner to convert the dictionary to the
        # format required for appending.
        row = pd.DataFrame([results]).to_records().astype(self.trial_log_dtype)
        self.trial_log.append(row)

    def process_event(self, event, timestamp):
        data = {'event': event, 'timestamp': timestamp}
        row = pd.DataFrame([data]).to_records().astype(self.event_log_dtype)
        self.event_log.append(row)

    def process_ai(self, name, data):
        if self._arrays[name] is not None:
            self._arrays[name].append(data)


def set_node(event):
    extension = find_extension(event.workbench, PLUGIN_ID, 'sink', HDFStore)
    if extension.initialized:
        raise ValueError('Cannot change node once data has initialized')
    extension.node = event.parameters['node']


from enaml.widgets.api import Container, Label

enamldef ChacoDockItem(DockItem):

    name = 'chaco_dock_item'
    closable = False

    attr plot

    Container:
        EnableCanvas:
            component << plot


def contribute_to_workspace(workbench, workspace):
    from psi.core.chaco.api import ChannelDataRange
    from chaco.api import LinearMapper, OverlayPlotContainer

    extension = find_extension(event.workbench, PLUGIN_ID, 'sink', HDFStore)
    index_range = ChannelDataRange(trig_delay=0)
    index_mapper = LinearMapper(range=index_range)

    container = OverlayPlotContainer(padding=[20, 20, 50, 5])

    item = ChacoDockItem(workspace.dock_area, plot=container)
    op = InsertItem(item=item.name, position='right')
    deferred_call(workspace.dock_area.update_layout, op)


enamldef HDFStoreManifest(PluginManifest): manifest:

    id = PLUGIN_ID

    Extension:
        id = 'sink'
        point = 'psi.data.sink'
        HDFStore:
            pass

    Extension:
        id = 'commands'
        point = 'enaml.workbench.core.commands'
        Command:
            id = PLUGIN_ID + '.set_node'
            handler = set_node

    Extension:
        id = 'workspace'
        point = 'psi.experiment.workspace'
        factory = contribute_to_workspace
